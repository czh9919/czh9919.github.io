---
title: 与学习有关的技巧
catalog: true
date: 2018-08-31 21:25:06
mathjax: true
subtitle:
header-img:
tags: Machine Learning
---

# 与学习有关的技巧

## 参数的更新

在之前，我们一直使用随机梯度下降法(SGD),来更新参数，不过SGD有一定的缺点，他非常低效，搁我们这说就是磨叽，所以，我们引入一个新的方法，叫Monmentum

### SGD

我们先看下SGD的表达式，方便在此基础上拓展
$$
W\leftarrow W - \eta\frac{\partial L}{\partial W}
$$

### Monmentum

$$
v\leftarrow \alpha v - \eta\frac{\partial L}{\partial W} 
$$


$$
W\leftarrow W+v
$$
和之前的SGD基本类似。
我们在代码中新建一个类，这样更方便，
代码见于[此](https://github.com/czh9919/Study-notes/blob/master/common/optimizer.py)

### AdaGrad

$$
h\leftarrow h+\frac{\partial L}{\partial W}\bigodot\frac{\partial L}{\partial W}
$$


$$
W\leftarrow W-\eta\frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}
$$

这是我认为稍微难一点的算法，做一些讲解，和SGD类似，\(W\)表示要更新的权重参数，\(\frac{\partial L}{\partial W}\)表示损失函数关于\(W\)的梯度,\(\eta\)表示学习率。\(h\)则用来存储以前所有梯度值的平方和(其中\( \bigodot\)表示对应矩阵元素的乘法)，这样每次更新参数，就能调节学习的尺度。
我打算看完这本书之后，看看哪本书能讲清楚原因的，这里看的知其然不知其所以然
AdaGrad见于[AdaGrad](https://github.com/czh9919/Study-notes/blob/master/common/optimizer.py)

### Adam

Adam是一种把AdaGrad和Momentum融合起来的做法，这里不说明了

## 权重的初始值

权重的初始值一般不设为0，甚至不能相同，为了防止“权重均一化”

### 隐藏层的激活层的分布

如果采用随机值作为初始值的话，就容易产生**梯度消失**或**表现力受限**的问题，这里，我们采用**Xavier初始值**
在Xavier的论文中，为了使各层的激活值呈现出相同的广度，推导了合适的权重尺度。结论为，如果前一层的节点数为\(n\)，则初始值使用标准差为\(\frac{1}{\sqrt{n}})\\
假设所有层的节点数都是100代码为

    node_num=100
    W=np.random.randn(node_num,node_num)/np.sqrt(node_num)

### ReLu的权重初始值

Xavier函数是以线性函数为前提推导出来的，因为sigmoid函数和tehn函数左右对称，且中央附近可以视作线性函数，可以使用Xavier初始值，而对于ReLu函数，则建议使用**He初始值**。He初始值就是使用标准差为\( \frac{2}{\sqrt{n}}\)的高斯分布

## Batch Normalization

### Batch Normalization 的算法

Batch Normalization有以下好处

+ 可以加快学习
+ 不是很依赖初始值
+ 抑制过拟合

那么什么是Batch Normalization呢？

$$
\mu\leftarrow\frac{1}{m}\sum^m_{i=1}x_i
$$


$$
\sigma_B^2\leftarrow\frac{1}{m}\sum^m_{i=1}(x_i-\mu_B)^2
$$


$$
\hat x_i \leftarrow \frac{x_i-\mu_B}{\sqrt{\sigma_B^2-\varepsilon}}
$$


$$
y_i\leftarrow \gamma\hat x_i+\beta
$$
一开始\(\gamma\)=1,\(\beta\)=0,然后再通过学习调整到合适的值

## 正则化

### 过拟合

主要有俩发生过拟合的原因

+ 模型拥有大量参数，表现力强
+ 训练数据少

### 权值衰减

权值衰减一般可以用来抑制过拟化,这里不细讲

### Dropout

Dropout更简单，就是随机删除隐藏层神经元

## 超参数的验证

略