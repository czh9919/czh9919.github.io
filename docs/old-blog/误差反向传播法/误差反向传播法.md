---
title: 误差反向传播法
catalog: true
date: 2018-08-28 17:11:08
mathjax: true
subtitle:
header-img:
tags: Machine Learning
---

# 误差反向传播法

个人认为有两种方法来正确理解误差反向传播法，一种是基于数学式，另一种是基于**计算图**这里我们以计算图来理解误差反向传播法

## 计算图

计算图将计算过程用图形表示出来。
在计算图解题的过程中，需要如下流程进行

1. 构建计算图
2. 在计算图上，从左至右进行计算(正向传播)

这是Google上搜索到的对计算图的定义
> A computational graph is a directed graph where the nodes correspond to operations or variables. Variables can feed their value into operations, and operations can feed their output into other operations. This way, every node in the graph defines a function of the variables.

更直观一些，拿一张图来说明
<!--  -->
有了计算图，我们可以通过正向传播和反向传播高效地计算各个变量的导数值

### 反向传播

个人认为这是一个非常让人感到困惑的概念，但通过他的原理**链式法则**,我们能比较清爽的了解反向传播的概念
这里我们引入一张图进行解释
<!--  -->
注意到这张图的正向传播是由下到上的，所以反向传播就是由上到下的
那么\( \frac{ \partial e }{\partial a} =2\) \( \frac{ \partial e }{\partial b} =5\),对\( \frac{ \partial e }{\partial a} =2\)就是(e-c-a)路径偏导值的乘积，对\( \frac{ \partial e }{\partial b} =5\)就是路径(e-c-b)偏导值的乘积和(e-d-b)偏导值的乘积之和。还记得高数老师曾在讲链式法则的时候，画过图，就是计算图，所以反向传播的本质就是链式法则。

### 简单层的实现

接下来我将进行简单层的实现

#### 乘法层的实现

对层的实现，我们有两个共同的方法也可以叫接口，forward()和backward(),其中forward对应正向传播，backward对应反向传播。我们把乘法层命名为MulLayer类。

    class mulLayer:
        def __init__(self):
            self.x=None
            self.y=None
        def forward(self,x,y):
            self.x=x
            self.y=y
            out=x*y
            return out
        def backward(self,dout):
            dx=dout*self.y
            dy=dout*self.x
            return dx,dy
代码见[乘法层的实现](https://github.com/czh9919/Study-notes/blob/master/Deep_Learning/MulLayer.py)

#### 加法层的实现

加法层的实现相对简单
代码见[加法层的实现](https://github.com/czh9919/Study-notes/blob/master/Deep_Learning/AddLayer.py)

### 激活函数层的实现

#### ReLu层

代码见[ReLu](https://github.com/czh9919/Study-notes/blob/master/common/layers.py)中的ReLu类

#### sigmoid层

代码见[sigmoid](https://github.com/czh9919/Study-notes/blob/master/common/layers.py)中的Sigmoid类
